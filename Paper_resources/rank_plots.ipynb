{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANK PLOTTER FOR BN NETWORKS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import rankdata, friedmanchisquare\n",
    "from Orange.evaluation import compute_CD, graph_ranks\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "datasets = [\"ecoli70\",\"magic_niab\",\"magic_irri\",\"arth150\",\"healthcare\",\"sangiovese\", \"mehra\"]\n",
    "for dataset_name in datasets:\n",
    "\n",
    "    file_path = \"save/save_\" + dataset_name + \"/\" + dataset_name + \"_CER.csv\" # WRITE \"CER\" OR \"UNC\" OR \"TP\" DEPENDING OF THE SCORE YOU WANT \n",
    "\n",
    "    dicc = pd.read_csv(file_path, usecols=lambda column: column != 'Unnamed: 0')\n",
    "   \n",
    "    df = pd.concat([df, dicc], ignore_index=True)\n",
    "    \n",
    "\n",
    "# Example performance results of classifiers on different datasets\n",
    "performance_data = df.to_numpy()\n",
    "\n",
    "print(df)  \n",
    "# Rank the classifiers for each dataset\n",
    "ranks = np.array([rankdata(p) for p in performance_data]) # IF SCORE IS \"CER\" write \"rankdata(-p)\"\n",
    "print(f\"Ranks:\\n{ranks}\")\n",
    "\n",
    "# Compute the average ranks of the classifiers\n",
    "average_ranks = np.mean(ranks, axis=0)\n",
    "print(f\"Average ranks:\\n{average_ranks}\")\n",
    "\n",
    "# Perform the Friedman test\n",
    "_, p_value = friedmanchisquare(*performance_data.T)\n",
    "print(f\"Friedman test p-value: {p_value}\")\n",
    "\n",
    "# Compute the critical difference\n",
    "cd = compute_CD(average_ranks, n=len(performance_data), alpha='0.05')\n",
    "print(f\"Critical Difference: {cd}\")\n",
    "\n",
    "# Prepare names with ranks\n",
    "names = [f\"{ele}\" for ele, rank in zip(df.columns, average_ranks)]\n",
    "\n",
    "# Plot the CD diagram\n",
    "graph_ranks(average_ranks, names=names, cd=cd, width=10, textspace=1.5,  reverse=True)\n",
    "\n",
    "\n",
    "#plt.savefig('save/save_PLOTS/CD_UNC_BN.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANK PLOTTER FOR REAL WORLD DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import rankdata, friedmanchisquare\n",
    "from Orange.evaluation import compute_CD, graph_ranks\n",
    "\n",
    "\n",
    "df_KNN = pd.DataFrame()\n",
    "df_SVM = pd.DataFrame()\n",
    "datasets = [\"ionosphere\",\"sonar\",\"myocardial\",\"spambase\",\"toxicity\", \"breast\", \"IMV_ABA\"]\n",
    "\n",
    "for dataset_name in datasets:\n",
    "\n",
    "    file_path = \"save/save_\" + dataset_name + \"/Best_conformal_scores\" + dataset_name + \".csv\"\n",
    "\n",
    "    dicc = pd.read_csv(file_path)\n",
    "    dicc = dicc.drop(columns=['P', 'P.1', 'P.2', 'P.3'])\n",
    "    \n",
    "\n",
    "    dicc = dicc.set_index(dicc.columns[0])\n",
    "    dicc.index = dicc.index.astype(str)\n",
    "      \n",
    "    dicc_KNN = dicc[dicc.index.str.contains(\"KNN\")]\n",
    "    dicc_SVM = dicc[dicc.index.str.contains(\"SVM\")]\n",
    "\n",
    "\n",
    "    df_SVM = pd.concat([df_SVM, dicc_SVM .T], ignore_index=False)\n",
    "    df_KNN = pd.concat([df_KNN, dicc_KNN.T], ignore_index=False)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "performance_scores = [\"inefficiency\", \"certainty\"]\n",
    "for name in performance_scores:\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df = df_KNN[df_KNN.index.str.contains(rf'\\b{name}\\b')] # CHANGE BETWEEN \"KNN\" AND \"SVM\" HERE:  \"df_SVM[df_SVM.index.str.contains(rf'\\b{name}\\b')]\"\n",
    "    print(df)\n",
    "    # Example performance results of classifiers on different datasets\n",
    "    performance_data = df.to_numpy()\n",
    "\n",
    "      \n",
    "    # Rank the classifiers for each dataset\n",
    "    if name == \"inefficiency\":\n",
    "        ranks = np.array([rankdata(p) for p in performance_data])\n",
    "    else:\n",
    "        ranks = np.array([rankdata(-p) for p in performance_data])\n",
    "    print(f\"Ranks:\\n{ranks}\")\n",
    "    \n",
    "    # Compute the average ranks of the classifiers\n",
    "    average_ranks = np.mean(ranks, axis=0)\n",
    "    print(f\"Average ranks:\\n{average_ranks}\")\n",
    "\n",
    "# Perform the Friedman test\n",
    "    _, p_value = friedmanchisquare(*performance_data.T)\n",
    "    print(f\"Friedman test p-value: {p_value}\")\n",
    "\n",
    "    # Compute the critical difference\n",
    "    cd = compute_CD(average_ranks, n=len(performance_data), alpha='0.05')\n",
    "    print(f\"Critical Difference: {cd}\")\n",
    "\n",
    "    # Prepare names with ranks\n",
    "    names = [f\"{ele}\" for ele, rank in zip(df.columns, average_ranks)]\n",
    "\n",
    "    # Plot the CD diagram\n",
    "    graph_ranks(average_ranks, names=names, cd=cd, width=10, textspace=1.5,  reverse=True)\n",
    "\n",
    "\n",
    "    plt.savefig('save/save_PLOTS/CD_'+ name +'_KNN.pdf')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STABILITY RANK PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import rankdata, friedmanchisquare\n",
    "from Orange.evaluation import compute_CD, graph_ranks\n",
    "\n",
    "\n",
    "datasets = [\"ionosphere\",\"sonar\",\"myocardial\",\"spambase\", \"toxicity\", \"breast\", \"IMV_ABA\"]\n",
    "list_of_scores = [ \"Nogue\"]\n",
    "list_of_methods = [ \"mRMR_MS_linear\" , \"mRMR_MS_poly\",  \"mRMR_MS_rbf\", \"mRMR\" ] # INCLUDE AT THE END OF THE LIST: \"JMI\", \"mRMR\", \"relax_mRMR\"\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))  # 1 row, 2 columns\n",
    "\n",
    "for score in list_of_scores:\n",
    "\n",
    "    stacked_arrays = [] \n",
    "    for dataset_name in datasets:\n",
    "\n",
    "        performance_of_methods = np.array([])\n",
    "        for method in list_of_methods:\n",
    "                \n",
    "            file_path = \"save/save_\" + dataset_name + \"/\" + \"STABILITY_\" +  dataset_name + \"_\" +  method + \".csv\"\n",
    "\n",
    "            csv = pd.read_csv(file_path)        \n",
    "            performance_data = csv[score].to_numpy()\n",
    "            x = [*range(1 , 50)]\n",
    "\n",
    "               \n",
    "            if performance_of_methods.size == 0:\n",
    "                performance_of_methods = performance_data\n",
    "                    \n",
    "            else:\n",
    "                performance_of_methods = np.vstack((performance_of_methods, performance_data))\n",
    "                \n",
    "        \n",
    "        # Rank the classifiers for each dataset\n",
    "            \n",
    "        rank = np.array([rankdata(-performance_of_methods[:, i]) for i in range(performance_of_methods.shape[1])])\n",
    "                \n",
    "        if rank.shape[0] != 50:  \n",
    "            new_array = np.full((50,len(list_of_methods)), np.nan)\n",
    "            new_array[:rank.shape[0], :rank.shape[1]] = rank\n",
    "            rank = new_array\n",
    "            \n",
    "        stacked_arrays.append(rank)\n",
    "\n",
    "    # Stack all arrays along a new axis to form a 3D array\n",
    "    rank_of_datasets = np.stack(stacked_arrays, axis=0)\n",
    "    average_ranks = np.nanmean(rank_of_datasets, axis=0)\n",
    "            \n",
    "        \n",
    "\n",
    "    # Assuming average_ranks is a 2D numpy array\n",
    "    num_columns = average_ranks.shape[1]  # Get the number of columns\n",
    "\n",
    "    colors = [  'm', 'y', 'k', 'b', 'g', 'r']\n",
    "    markers = [ 'D', 'v', '<', '>', 'o', 's', '^']\n",
    "    linestyles = ['-', '--', '-.', ':', '-', '--', '-.']\n",
    "\n",
    "    for i in range(num_columns):\n",
    "        ax.plot(average_ranks[:, i], color=colors[i], marker=markers[i], linestyle=linestyles[i], label=f'{list_of_methods[i]}')\n",
    "\n",
    "    # Adding labels and title for the first plot\n",
    "\n",
    "# Set labels, title, and other properties for the single plot\n",
    "    ax.set_xlabel('Top ranked features', fontsize=14)\n",
    "    ax.set_ylabel('Average Rank', fontsize=14)\n",
    "    ax.set_title('Stability', fontsize=14)\n",
    "    ax.set_ylim(0.85, 6.1)  # Ensure y-axis is between 0 and 7\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "\n",
    "\n",
    "# Create a single legend for both plots\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.005), ncol=6, fontsize=13)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0.01, 0.08, 1, 1]) \n",
    "\n",
    "plt.savefig('save/save_PLOTS/STABILITY_ranks_mRMR.pdf')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
